---
title: "Determinants Elasticity xgbTree"
output:
  html_document: default
  pdf_document: default
date: "2023-04-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Preliminaries: load required packages and perform merges

```{r, echo=FALSE}
rm(list = ls())
library(tidyverse)
library(weights)
library(caret)
library(xgboost)
library(doParallel)
library(pdp)
options(width = 300)
```

# create weight manually
```{r}
rm(list = ls())
library(tidyverse)
library(readxl)
filenames <- list.files("./Ranked Measure Data2")
filenames
RMD <- data.frame(matrix(ncol = 0, nrow = 0))
for (filename in filenames){  
   yearnum <- gsub(".xls", "", filename)  
   RMD = RMD %>% bind_rows(assign(paste0("RMD",yearnum), read_excel(paste0("./Ranked Measure Data2/", filename), sheet="Ranked Measure Data", skip = 1) %>%
            mutate(year=yearnum) %>% 
            select(FIPS, year, YPLLRateLow, YPLLRateHigh)))
}

RMD$year=as.integer(RMD$year)
RMD=RMD %>% filter(substring(FIPS, 3,5)!="000")
RMD=RMD %>% drop_na(FIPS)
RMDwide= pivot_wider(RMD, id_cols = FIPS,  names_from = year, values_from = c(YPLLRateLow, YPLLRateHigh))

RMDwide$range15=(RMDwide$YPLLRateHigh_2015 - RMDwide$YPLLRateLow_2015)/(4*1.96)
RMDwide$irange15=1/RMDwide$range15
RMDwide$weight15=RMDwide$irange15 / sum(RMDwide$irange15, na.rm = TRUE)
sum(RMDwide$weight15 , na.rm = TRUE)

RMDwide$range16=(RMDwide$YPLLRateHigh_2016 - RMDwide$YPLLRateLow_2016)/(4*1.96)
RMDwide$irange16=1/RMDwide$range16
RMDwide$weight16=RMDwide$irange16 / sum(RMDwide$irange16, na.rm = TRUE)
sum(RMDwide$weight16, na.rm = TRUE)

RMDwide$range17=(RMDwide$YPLLRateHigh_2017 - RMDwide$YPLLRateLow_2017)/(4*1.96)
RMDwide$irange17=1/RMDwide$range17
RMDwide$weight17=RMDwide$irange17 / sum(RMDwide$irange17, na.rm = TRUE)
sum(RMDwide$weight17, na.rm = TRUE)

RMDwide$range18=(RMDwide$YPLLRateHigh_2018 - RMDwide$YPLLRateLow_2018)/(4*1.96)
RMDwide$irange18=1/RMDwide$range18
RMDwide$weight18=RMDwide$irange18 / sum(RMDwide$irange18, na.rm = TRUE)
sum(RMDwide$weight18, na.rm = TRUE)

RMDwide$range19=(RMDwide$YPLLRateHigh_2019 - RMDwide$YPLLRateLow_2019)/(4*1.96)
RMDwide$irange19=1/RMDwide$range19
RMDwide$weight19=RMDwide$irange19 / sum(RMDwide$irange19, na.rm = TRUE)
sum(RMDwide$weight19, na.rm = TRUE)

weight=RMDwide[c(1, 14, 17, 20, 23, 26)]
weight[is.na(weight)] = 0
weight$averweight=dim(weight)[1]*(weight$weight15 + weight$weight16 + weight$weight17 + weight$weight18 + weight$weight19)/5
weight=weight[c(1, 7)]
save(weight, file = "weight.RData")

```

Perform merges: 

```{r, echo=FALSE}
disparity=read_csv("./race.csv", col_types = "ccdddd")
disparity$FIPS=substr(disparity$GEO_ID, 10, 14)
disparity$blwt=disparity$S1701_C03_010E - disparity$S1701_C03_009E
disparity$hpnon=disparity$S1701_C03_016E - disparity$S1701_C03_017E
tomerge=disparity[c(7:9)]
load("analytic3FIPS.RData")
analyticwrace=merge(analytic3, tomerge, by.x="FIPS", by.y="FIPS")
load("weight.RData")
analyticwrace=merge(analyticwrace, weight, by.x="FIPS", by.y="FIPS")
analyticwrace = na.omit(analyticwrace)
#YPLLanalytic=analyticwrace[-c(2,3,4,23)]
YPLLanalytic=analyticwrace %>% select(-FIPS,-averweight)
save(YPLLanalytic, file="YPLLanalytic.Rdata")
save(analyticwrace, file="analyticwrace.Rdata")
```

Tune xgbTree:

```{r}
load(file = "YPLLanalytic.Rdata")

nc = parallel::detectCores()  
cl = makePSOCKcluster(nc-1)   # Set number of cores equal to machine number minus one
registerDoParallel(cl)        #Set up parallel

fitControl = trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 10,
                          allowParallel=TRUE
)

#Step 1: find range of nrounds associated with different learning rates with non-stochastic trees, default max_depth 6, min_child_weight 20

# xgbGrid1 = expand.grid(nrounds = c(1,5,10,20,50,100,150,200,250,300,350,400,450,500,550,600,650,700,750,800,850,900,950,1000,1100,1200,1300,1400,1500), 
#                        max_depth = 6, 
#                        eta = c(.0025,.005,.01,.02,.04,.08,.16,.32), 
#                        gamma = 0,
#                        colsample_bytree = 1, 
#                        min_child_weight = 20,
#                        subsample = 1) 
# 
# set.seed (112358)
# xgbFit1 = train(YPLLdif ~ ., data = YPLLanalytic,
#                 method = "xgbTree", 
#                 trControl = fitControl,
#                 tuneGrid = xgbGrid1,
#                 weights = analyticwrace$averweight,
#                 nthread=1,
#                 verbosity = 0
# )
# 
# plot(xgbFit1)
# xgbFit1$bestTune
# 
# #Step 2: We've ruled out fast learning rates. .005 and .01 are approximately the same. Next, allow for stochastic variation and see if range still works
# 
# xgbGrid2 = expand.grid(nrounds = seq(100,1000,100), 
#                        max_depth = 6, 
#                        eta = c(.005,.01), 
#                        gamma = 0,
#                        colsample_bytree = c(.2,.5,.8,1), 
#                        min_child_weight = 20,
#                        subsample = c(.2,.5,.8,1)) 
# 
# set.seed (112358)
# xgbFit2 = train(YPLLdif ~ ., data = YPLLanalytic,
#                 method = "xgbTree", 
#                 trControl = fitControl,
#                 tuneGrid = xgbGrid2,
#                 weights = analyticwrace$averweight,
#                 nthread=1,
#                 verbosity = 0
# )
# 
# plot(xgbFit2)
# xgbFit2$bestTune
# 
# #Step 3: eta = .005 confirmed around 600 trees; approx subsample .2, colsample .5; now vary depth
# 
# xgbGrid3 = expand.grid(nrounds = seq(400,1200,50), 
#                        max_depth = c(3,4,5,6,7,8,9), 
#                        eta = .005, 
#                        gamma = 0,
#                        colsample_bytree = c(.4,.5,.6,.7), 
#                        min_child_weight = 20,
#                        subsample = c(.1,.2,.3)) 
# 
# set.seed (112358)
# xgbFit3 = train(YPLLdif ~ ., data = YPLLanalytic,
#                 method = "xgbTree", 
#                 trControl = fitControl,
#                 tuneGrid = xgbGrid3,
#                 weights = analyticwrace$averweight,
#                 nthread=1,
#                 verbosity = 0
# )
# 
# plot(xgbFit3)
# xgbFit3$bestTune

#Step 4: final fine-tuning / subsample .2, colsample .5, depth 7, around 600 trees

xgbGridFinal = expand.grid(nrounds = seq(500,700,2), 
                       max_depth = 7, 
                       eta = .005, 
                       gamma = 0,
                       colsample_bytree = .5, 
                       min_child_weight = 20,
                       subsample = .2) 

set.seed (112358)
xgbFitFinal = train(YPLLdif ~ ., data = YPLLanalytic,
                method = "xgbTree", 
                trControl = fitControl,
                tuneGrid = xgbGridFinal,
                weights = analyticwrace$averweight,
                nthread=1,
                verbosity = 0
)

plot(xgbFitFinal)
xgbFitFinal$bestTune

stopCluster(cl)

save(xgbFitFinal,file="xgbFitFinal.Rdata")
```

Results:

```{r}
load(file="YPLLanalytic.Rdata")
load(file = "xgbFitFinal.Rdata")

plot(varImp(xgbFitFinal))
vi=varImp(xgbFitFinal)
vi$importance
xgbFitFinal
xgb.pdp = list()
res.partialplot = list()
predvarls = rownames(varImp(xgbFitFinal)$importance)

for (m in 1:length(predvarls)){
  xgb.pdp[[m]] = 
    partial(
      object = xgbFitFinal,
      pred.var = predvarls[[m]],
      plot = FALSE,
      chull = TRUE,
      plot.engine = "ggplot2"
    )
  res.partialplot[[m]] = plotPartial(xgb.pdp[[m]], rug =TRUE, train = YPLLanalytic, ylim=c(99, 601))
}

for(j in 1:length(predvarls)){
  print(res.partialplot[[j]])
}

library(sfsmisc)
library(usmap)

res_e = numeric(length = length(predvarls))
res_semi_e = numeric(length = length(predvarls))
county_e = matrix(data = NA,nrow = dim(YPLLanalytic)[1],ncol = length(predvarls))
res.elastplot = list()
res.semi_elastplot = list()
goodlist = c("somecollegea","somecollegeb","PCPratea","PCPrateb","highschoola","highschoolb") #Variables where increase is hypothesized to reduce YPLLdif

for (j in 1:length(predvarls)) {
  sm_pdp_j = smooth.spline(x = xgb.pdp[[j]][, 1], y = xgb.pdp[[j]][, 2], df = 5) #Smooth partial dependency of YPLL on predictor j
  d_pdp_d_j = D1tr(y = sm_pdp_j$y, x = sm_pdp_j$x) #Derivative of smoothed partial dependency of YPLL on predictor j
  e_pdp_j = (sm_pdp_j$x / sm_pdp_j$y) * d_pdp_d_j #Elasticity of YPLL with respect to predictor j
  plot(
    sm_pdp_j$x,
    e_pdp_j,
    xlab = predvarls[j],
    ylab = "Elasticity of YPLLdif",
    ylim = c(-1, 1)
  )
  interp.elast = approxfun(x = sm_pdp_j$x, y = e_pdp_j)
  res_e[j] = weighted.mean(x = interp.elast(YPLLanalytic[, predvarls[j]]), w =
                             analyticwrace$averweight)
  county_e[, j] = interp.elast(YPLLanalytic[, predvarls[j]])
  etest = data.frame(fips = analyticwrace$FIPS, elast = county_e[, j])
  # if (predvarls[j] %in% goodlist) {
  #   semi_e_j = -county_e[, j] * abs(YPLLanalytic$YPLLdif)
  # } else semi_e_j = county_e[, j] * abs(YPLLanalytic$YPLLdif)
  
  semi_e_j = county_e[, j] * abs(YPLLanalytic$YPLLdif)
  res_semi_e[j]=weighted.mean(x = county_e[, j] * abs(YPLLanalytic$YPLLdif), w =
                             analyticwrace$averweight)
  
  semi_etest = data.frame(fips = analyticwrace$FIPS, semi_elast = semi_e_j)
  res.elastplot[[j]] = plot_usmap(regions = "counties",
                                  data = etest,
                                  values = "elast") + scale_fill_gradientn(
                                    name="elasticity",
                                    colours = c("darkgreen", "green", "white", "red", "darkred"),
                                    na.value = "gray",
                                    breaks = c(-1,-.5, 0, .5, 1),
                                    labels = c(-1,-.5, 0, .5, 1),
                                    limits = c(-1, 1)
                                  ) + ggtitle(predvarls[j])
  print(res.elastplot[[j]])
  res.semi_elastplot[[j]] = plot_usmap(regions = "counties",
                                       data = semi_etest,
                                       values = "semi_elast") + scale_fill_gradientn(
                                         name="semi-elasticity",
                                         colours = c("darkgreen", "green", "white", "red", "darkred"),
                                         na.value = "gray",
                                         breaks = c(-3000,-1500, 0, 1500,3000),
                                         labels = c(-3000,-1500, 0, 1500,3000),
                                         limits = c(-3000, 3000)
                                       )+ ggtitle(predvarls[j])
  print(res.semi_elastplot[[j]])
}

print(cbind(predvarls,res_e)) #Overall elasticity results
print(cbind(predvarls,res_semi_e)) #semi elasticity results
```

